{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO//yWOQgo3fu1LE/VtHPCt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kq-1zCD4-HVg","executionInfo":{"status":"ok","timestamp":1701092596772,"user_tz":-330,"elapsed":112685,"user":{"displayName":"21ai016 Jagadeeswaran V P","userId":"06313423288146334397"}},"outputId":"ff6bf826-7a04-41f1-8388-aee68b23129a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","155/155 [==============================] - 3s 9ms/step - loss: 3.2774\n","Epoch 2/50\n","155/155 [==============================] - 1s 8ms/step - loss: 3.2290\n","Epoch 3/50\n","155/155 [==============================] - 1s 8ms/step - loss: 3.2073\n","Epoch 4/50\n","155/155 [==============================] - 1s 9ms/step - loss: 3.1389\n","Epoch 5/50\n","155/155 [==============================] - 1s 9ms/step - loss: 3.0537\n","Epoch 6/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.9853\n","Epoch 7/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.9181\n","Epoch 8/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.8527\n","Epoch 9/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.7950\n","Epoch 10/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.7599\n","Epoch 11/50\n","155/155 [==============================] - 1s 8ms/step - loss: 2.7311\n","Epoch 12/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.7118\n","Epoch 13/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6973\n","Epoch 14/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6803\n","Epoch 15/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6649\n","Epoch 16/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6556\n","Epoch 17/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6355\n","Epoch 18/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6262\n","Epoch 19/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.6110\n","Epoch 20/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.5948\n","Epoch 21/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.5798\n","Epoch 22/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.5592\n","Epoch 23/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.5385\n","Epoch 24/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.5185\n","Epoch 25/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.4957\n","Epoch 26/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.4711\n","Epoch 27/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.4393\n","Epoch 28/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.4121\n","Epoch 29/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.3796\n","Epoch 30/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.3436\n","Epoch 31/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.3109\n","Epoch 32/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.2708\n","Epoch 33/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.2298\n","Epoch 34/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.1875\n","Epoch 35/50\n","155/155 [==============================] - 2s 11ms/step - loss: 2.1399\n","Epoch 36/50\n","155/155 [==============================] - 1s 10ms/step - loss: 2.0975\n","Epoch 37/50\n","155/155 [==============================] - 1s 10ms/step - loss: 2.0459\n","Epoch 38/50\n","155/155 [==============================] - 1s 9ms/step - loss: 2.0010\n","Epoch 39/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.9744\n","Epoch 40/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.8980\n","Epoch 41/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.8496\n","Epoch 42/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.7948\n","Epoch 43/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.7513\n","Epoch 44/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.6889\n","Epoch 45/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.6588\n","Epoch 46/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.5690\n","Epoch 47/50\n","155/155 [==============================] - 2s 10ms/step - loss: 1.5357\n","Epoch 48/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.4785\n","Epoch 49/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.4282\n","Epoch 50/50\n","155/155 [==============================] - 1s 9ms/step - loss: 1.3727\n","Seed:\n","\" er to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is \"\n"," chiet tn tor rosnre to to civ ne toe soshrr  oo teah tour davesr  wo tiah tour  lhe t aisi io tor  tho hrast si tou,\n","Wh the merss it wer  thu hras, shot sell? shet s ghee \n","The  th toe mets, sou heas,\n","shu sre gitis sn tou \n","\n","MENENIUS:\n","Fht, thete are tlmtst th fofd so tore bil yhur bitese Tind thurrreve \n","\n","FENENIUS:\n","Iate tran,ss ge thit were arle'so dose bo  ane conynds  no teah tour -lh yhes wiu hase fetn rne toe thonse  wh cadh to tor \n","thu sra gitri st tuu \n","\n","niet say you te'l?\n","the tans, snu thens\n","Done.\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import sys  # Add this line to import the sys module\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","# Get the Shakespeare text data from TensorFlow\n","file_path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","\n","# Read the text data from the file\n","with open(file_path, 'r', encoding='utf-8') as file:\n","    shakespeare_text = file.read()\n","\n","# Extract a subset of the Shakespeare data (you can adjust the size as needed)\n","tech_text = shakespeare_text[:10000]  # Replace this with your tech-related text data\n","\n","# Create a mapping of unique characters to integers\n","chars = sorted(list(set(tech_text)))\n","char_to_int = {char: i for i, char in enumerate(chars)}\n","int_to_char = {i: char for i, char in enumerate(chars)}\n","\n","# Convert the text to integer sequences\n","seq_length = 100  # You can adjust this based on your preference\n","dataX = []\n","dataY = []\n","for i in range(0, len(tech_text) - seq_length, 1):\n","    seq_in = tech_text[i:i + seq_length]\n","    seq_out = tech_text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])\n","\n","# Reshape the data\n","X = np.reshape(dataX, (len(dataX), seq_length, 1))\n","X = X / float(len(chars))\n","y = tf.keras.utils.to_categorical(dataY)\n","\n","# Build the model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","# Train the model\n","model.fit(X, y, epochs=50, batch_size=64)\n","\n","# Generate text\n","start = np.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","\n","# Generate characters\n","for i in range(500):\n","    x = np.reshape(pattern, (1, len(pattern), 1))\n","    x = x / float(len(chars))\n","    prediction = model.predict(x, verbose=0)\n","    index = np.argmax(prediction)\n","    result = int_to_char[index]\n","    seq_in = [int_to_char[value] for value in pattern]\n","    sys.stdout.write(result)\n","    pattern.append(index)\n","    pattern = pattern[1:len(pattern)]\n","\n","print(\"\\nDone.\")\n"]}]}